# -*- coding: utf-8 -*-
"""Traditional_DLNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b1JbN5zMwg8EKCynaaFEYMDsZIScCK0l
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load the dataset into a pandas DataFrame
train_file_path = '/content/drive/MyDrive/LLM/training.csv'
df = pd.read_csv(train_file_path)

# Check if 'category' column exists in the DataFrame
if 'category' in df.columns:
    # Separate the features (inputs) and the labels
    X = df.drop(['category'], axis=1)  # Exclude the 'category' column
    y = df['category']
else:
    print("Column 'category' not found in the DataFrame.")
    X = df.copy()  # Use all columns as features
    y = None  # No labels available

# Encode categorical columns
categorical_cols = ['proto', 'saddr', 'sport', 'daddr', 'dport', 'state_number', 'attack', 'subcategory']
for col in categorical_cols:
    if col in X.columns:
        X[col] = LabelEncoder().fit_transform(X[col])
    else:
        print(f"Column {col} not found in the DataFrame.")

# Convert non-numeric columns to numeric types
X = X.apply(pd.to_numeric, errors='coerce')
X = X.fillna(0)  # Replace NaN values with 0

# Convert the labels to numeric values if available
if y is not None:
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Split the dataset into training and testing sets
if y is not None:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create a sequential model
    model = Sequential()

    # Input layer
    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))

    # Hidden layers
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(32, activation='relu'))

    # Output layer
    model.add(Dense(len(np.unique(y)), activation='softmax'))  # Number of unique labels

    # Compile the model
    model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(X_train, y_train, epochs=7, batch_size=32)

    # Evaluate the model on the test set
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.4f}")

    # Extract and print weights and biases
    for i, layer in enumerate(model.layers):
        weights = layer.get_weights()
        if len(weights) > 0:
            print(f"Layer {i+1} - {layer.name}")
            print("Weights:\n", weights[0])
            print("Biases:\n", weights[1])
        else:
            print(f"Layer {i+1} - {layer.name} has no weights or biases.")


# Save the trained model
model.save('/content/trained_model.h5')  # Save the model in the current directory or specify your desired save path

# Get the file path of the saved model
import os
model_path = os.path.abspath('/content/trained_model.h5')
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import load_model

# Load the testing dataset into a pandas DataFrame
test_file_path = '/content/testing_small.csv'
test_df = pd.read_csv(test_file_path)

# Perform the same data preprocessing steps as done for the training dataset
# Encode categorical columns
for col in categorical_cols:
    if col in test_df.columns:
        test_df[col] = LabelEncoder().fit_transform(test_df[col])
    else:
        print(f"Column {col} not found in the testing DataFrame.")

# Convert non-numeric columns to numeric types
test_df = test_df.apply(pd.to_numeric, errors='coerce')
test_df = test_df.fillna(0)  # Replace NaN values with 0

# Load the trained model
model_path = '/content/trained_model.h5'  # Specify the path where your trained model is saved
model = load_model(model_path)

# Make predictions on the testing dataset
predictions = model.predict(test_df)

# Get the predicted labels (categories) for the testing dataset
predicted_labels = np.argmax(predictions, axis=1)

# Print the top 10 predictions
top_10_predictions = predicted_labels[:10]
print("Top 10 Predictions:")
for prediction in top_10_predictions:
    print(prediction)